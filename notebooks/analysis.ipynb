{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and Transforming Citi Bike Data for Analysis \n",
    "## How is Citi Bike availability affected by various factors like time of day and weather?\n",
    "\n",
    "### Who:\n",
    "I am [Alhan Keser](https://blog.alhan.co/), a [10+ year specialist in Web Experimentation](https://www.linkedin.com/in/alhankeser/) (aka A/B Testing, Conversion Optimization), on my way to a Master's in Data Science.\n",
    "\n",
    "### What:\n",
    "This is an original analysis of Citi Bike station data from May-June 2019 to find out what affect the day of week, time of day, and weather (temperature, precipitation, etc...) have on the availability of bikes at station-,  neighborhood-, and borough-levels. \n",
    "\n",
    "### Why:\n",
    "- I wanted to push myself to extract and transform my own data. Skipping the entire ETL process and going straight into analysis is a luxury: it does not reflect reality. \n",
    "- Doing a time-series analysis is something that I wanted practice with. \n",
    "- I commute by bike every day (despite weather) so I have first-hand evidence that Citi Bike riders tend to shy away from biking in inclement weather. It will be interesting to visualize the differences here.   \n",
    "\n",
    "### How:\n",
    "- **Combined original data sources:**\n",
    "    - [Citi Bike Live Station Status](https://feeds.citibikenyc.com/stations/stations.json)\n",
    "    - [Dark Sky Weather API](https://darksky.net/dev/docs)\n",
    "    - [Google Geocoding API](https://developers.google.com/maps/documentation/geocoding/intro)\n",
    "- **Created cron jobs** to collect Citi Bike station statuses for all ~858 stations, every 3 minutes, for ~2 months.\n",
    "    - Total rows in final table: 5,800,274\n",
    "    - \"Why stop after 2 months,\" you ask? Because my server ran out of space while I was on vacation. Here's what that looks like: \n",
    "\n",
    "![My server crashed July 14](https://blog.alhan.co/storage/images/posts/2/web-server-crashed_2_1568434613_sm.jpg)\n",
    "- **Created a mini-ETL process** to transform data into the final output used below. \n",
    "    - Along the way, there were many errors, some of which I will resolve here.\n",
    "\n",
    "### Table of Contents\n",
    "- [Packages](#Packages)\n",
    "- [Extracting](#Extracting)\n",
    "    - [Stations-Raw](#Stations-Raw)\n",
    "    - [Stations-Flat](#Stations-Flat)\n",
    "    - [Geocoding](#Geocoding)\n",
    "    - [Weather](#Weather)\n",
    "    - [Cron Jobs](#Cron-Jobs)\n",
    "- [Transforming](#Transforming)\n",
    "    - [Availability by Station](#Availability-by-Station)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "    - [Hypotheses](#Hypotheses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "Importing a few packages that will help with describing, cleaning and visualizing things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from darksky import forecast\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from dateutil.parser import parse\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting\n",
    "I started by find an interesting data source. In this case, I found the [Citi Bike Station Feed](https://feeds.citibikenyc.com/stations/stations.json) via the [NYC Open Data site](https://opendata.cityofnewyork.us/).\n",
    "\n",
    "The feed shows the latest statuses of ~858 Citi Bike stations. Below is a list of values per station and sample data for each. Any keys left blank are often blank in the data source as well, which I'll address in later steps. \n",
    "\n",
    "| key | sample value |\n",
    "|------------:|:---------|\n",
    "| `id`        | 285|\n",
    "| `stationName` |\"Broadway & E 14St\"|\n",
    "| `availableDocks` |20|\n",
    "| `totalDocks` |53|\n",
    "| `latitude`|40.73454567|\n",
    "| `longitude`   |-73.99074142|\n",
    "| `statusValue` |\"In Service\"|\n",
    "| `statusKey`   |1|\n",
    "| `availableBikes` |31|\n",
    "| `stAddress1`  |\"Broadway & E 14 St\"|\n",
    "| `stAddress2`  |\"\"|\n",
    "| `city`        |\"\"|\n",
    "| `postalCode`  |\"\"|\n",
    "| `location`    |\"\"|\n",
    "| `altitude`    |\"\"|\n",
    "| `testStation` |false|\n",
    "| `lastCommunicationTime` |\"2019-09-12 08:38:21 PM\"|\n",
    "| `landMark`    |\"\"|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stations-Raw\n",
    "\n",
    "To have a back-up in case any of the subsequent steps went awry, I wanted to store the source data in the simplest way possible: a table `stations_raw` that stored the following: \n",
    "\n",
    "|column_name|data_type|sample value|\n",
    "|-----------|-----------|----------|\n",
    "|id         |int4|31419|\n",
    "|status     |json|{\"executionTime\": \"2019-06-22 01:53:41 PM\", \"s...|\n",
    "\n",
    "Once the table created, I needed a way to collect data. A quick solution -- for me -- was to create [a Laravel application](https://github.com/alhankeser/citibike-tracker/)  that [makes it easy create console commands](https://laravel.com/docs/5.8/artisan#writing-commands). In combination with [Laravel Forge](https://forge.laravel.com), it's easy to set up a cron job that triggers [the necessary command](https://github.com/alhankeser/citibike-tracker/blob/d61f82adde88c90430205785297abf9f3de07c4d/app/Console/Kernel.php#L49) at set intervals.\n",
    "\n",
    "Once the commands created, I set up a [cron job](#Cron-Jobs) that ran once every 3 minutes. This resulted in the collection of 41,325 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stations-Flat\n",
    "As part of the same command that creates the [stations_raw](#Stations-Raw) table, I [flattened out the JSON](https://github.com/alhankeser/citibike-tracker/blob/d61f82adde88c90430205785297abf9f3de07c4d/app/Console/Kernel.php#L80) and created a table with a single row per 3-minute interval, per station. We'll call this table `stations_flat` (probably could have used a better naming convention throughout this project). \n",
    "\n",
    "Here is the structure of `stations_flat` and some sample data:\n",
    "\n",
    "|column_name|data_type|sample value|description|\n",
    "|-----------|---------|------------|-----------|\n",
    "|id         |int4     |10511778    |row id|\n",
    "|station_id |int4     |72          |unique id for each station|\n",
    "|available_bikes|int4 |4           |number of available bikes at the station|\n",
    "|available_docks|int4 |49          |number of available docks (places to park a bike) at the station|\n",
    "|station_status|text  |In Service  |whether the station is in or out of service|\n",
    "|last_communication_time|timestamp|2019-05-15 01:14:15|the last time the station sent back data|\n",
    "\n",
    "After just over 2 months of this, I ended up with **34,301,048 rows** in this table. Luckily, I took some steps to make the volume of data more manageable when analyzing outside of a high CPU/RAM environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stations-Static\n",
    "As the name suggests, `stations_static` contains information about each station that doesn't change minute-to-minute. Since there was a likelihood that stations be added, removed, renamed, I inserted or updated on duplicate each time `stations_flat` was updated. \n",
    "\n",
    "|column_name|data_type|sample value|description|\n",
    "|-----------|---------|------------|-----------|\n",
    "|id| int4|3119|unique `station_id` found throughout db|\n",
    "|name| text|Vernon Blvd & 50 Ave||\n",
    "|latitude |float8|40.74232744||\n",
    "|longitude |float8|-73.95411749||\n",
    "|status_key| int4|1||\n",
    "|postal_code| text|NULL||\n",
    "|st_address_1| text|Vernon Blvd & 50 Ave||\n",
    "|st_address_2| text|NULL||\n",
    "|total_docks| int4|45||\n",
    "|status| text|In Service||\n",
    "|altitude| text|NULL||\n",
    "|location| text|NULL||\n",
    "|land_mark| text|NULL||\n",
    "|city| text|NULL||\n",
    "|is_test_station| int4|0||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding\n",
    "As can be seen from the `stations_static` table above, many of the location-related values are null. This was the case for all stations. I wanted to be able to group stations by neighborhood and zip. Also, I wanted to use zip to associate weather data to each station, without having to make separate requests for each station (to stay within the free tier of the [Dark Sky Weather API](https://darksky.net/dev/docs)). \n",
    "\n",
    "To geocode from lat/long for each station into human-readeable location info, I used the [Google Geocoding API](https://developers.google.com/maps/documentation/geocoding/intro). [See the command I used to create the below table](https://github.com/alhankeser/citibike-tracker/blob/d61f82adde88c90430205785297abf9f3de07c4d/app/Console/Kernel.php#L92)\n",
    "\n",
    "|column_name|data_type|sample value|description|\n",
    "|-----------|---------|------------|-----------|\n",
    "|id|int4|1||\n",
    "|station_id|int4|3119|unique station id|\n",
    "|zip|text|11101|zip code of station|\n",
    "|hood_1|text|LIC|neighborhood or the closest thing provided by Google|\n",
    "|hood_2|text|Hunters Point|another level of neighborhood|\n",
    "|borough|text|Queens|one of 5 NYC boroughs or New Jersey|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather\n",
    "Grouping stations by zip, I then called the [Dark Sky Weather API](https://darksky.net/dev/docs) once every hour to build the `weather` table. Reducing the scope to zip made it possible to stay within the free plan limits of Dark Sky. \n",
    "\n",
    "|column_name|data_type|sample value|description|\n",
    "|-----------|---------|------------|-----------|\n",
    "|id|int4|1||\n",
    "|time_interval|timestamptz|2019-05-02 01:00:00-04|the 15-minute interval of time to associate the weather data to|\n",
    "|summary|text|Foggy|a categorical label for weather conditions|\n",
    "|precip_intensity|float8|0|percent percipitation intensity|\n",
    "|temperature|float8|61.45|temperature in Fahrenheit|\n",
    "|apparent_temperature|float8|61.89|\"feels-like\" temperature|\n",
    "|dew_point|float8|60.82||\n",
    "|humidity|float8|0.98|percent humidity|\n",
    "|wind_speed|float8|3.11|speed in MPH|\n",
    "|wind_gust|float8|5.38|gusts in MPH|\n",
    "|cloud_cover|float8|1|percent cloud cover|\n",
    "|uv_index|float8|0||\n",
    "|visibility|float8|3.18||\n",
    "|ozone|float8|316.23||\n",
    "|status|text|observed|one of two values (\"predicted\"/\"observed\") depending on if the weather values are from the past or the future|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cron Jobs\n",
    "I'm not going to spend a lot of time on discussing cron jobs, but here are the patterns I was using to run everything. There is probably a more optimal approach that I am not aware of. \n",
    "\n",
    "|Cron       |Command|\n",
    "|-----------|-------|\n",
    "|\\*/3 \\* \\* \\* \\* | get:docks && update:availability 0 && update:weather|\n",
    "|0 \\*/2 \\* \\* \\*  | get:weather 0|\n",
    "\n",
    "View the code behind each command:\n",
    "- `get:docks` [view](https://github.com/alhankeser/citibike-tracker/blob/d61f82adde88c90430205785297abf9f3de07c4d/app/Console/Kernel.php#L49)\n",
    "- `update:availability 0` [view](https://github.com/alhankeser/citibike-tracker/blob/d61f82adde88c90430205785297abf9f3de07c4d/app/Console/Kernel.php#L179)\n",
    "- `update:weather` [view](https://github.com/alhankeser/citibike-tracker/blob/d61f82adde88c90430205785297abf9f3de07c4d/app/Console/Kernel.php#L359)\n",
    "- `get:weather 0` [view](https://github.com/alhankeser/citibike-tracker/blob/d61f82adde88c90430205785297abf9f3de07c4d/app/Console/Kernel.php#L276)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming\n",
    "Once I had the 1 raw table and 4 tables above updating as expected, I created a single, flat table that had the final data I intended to use for analysis. I kept most columns except some of the minutiae of the `weather` table. This table was updated regularly such that I could run analyses on an on-going basis and avoid having to run a massive, single query after all data had been collected.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Availability by Station\n",
    "Below is the flat table `availability` that combined the above tables, purpose-built for analysis. Note that available_bikes is the minimum number of bikes available during any of the 3-minute intervals during which samples were collected over the course of each 15-minute interval. \n",
    "\n",
    "|column_name|data_type|\n",
    "|-----------|---------|\n",
    "|time_interval|timestamptz|\n",
    "|station_id|int4|\n",
    "|station_name|text|\n",
    "|station_status|text|\n",
    "|latitude|float8|\n",
    "|longitude|float8|\n",
    "|zip|text|\n",
    "|borough|text|\n",
    "|hood|text|\n",
    "|available_bikes|int4|\n",
    "|available_docks|int4|\n",
    "|weather_summary|text|\n",
    "|precip_intensity|float8|\n",
    "|temperature|float8|\n",
    "|humidity|float8|\n",
    "|wind_speed|float8|\n",
    "|wind_gust|float8|\n",
    "|cloud_cover|float8|\n",
    "|weather_status|text|\n",
    "|created_at|timestamptz|\n",
    "|updated_at|timestamptz|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "On to the fun part! Now that I've got all of my station-by-station availability by 15-minute interval, it's time to explore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypotheses\n",
    "\n",
    "Below is a list of hypotheses, in no particular order, that may be interesting to validate in the following analysis:  \n",
    "- If we categorize stations by 'residential', 'business', and 'touristic' we will see distinctly different availability patterns.\n",
    "- On business days, rush hour traffic will significantly impact availability at 'residential' and 'business' stations, whereas 'touristic' stations will fluctuate throughout the day and may not have a repeating pattern.\n",
    "- Stations deemed 'touristic' will be more affected by changes in weather than non-touristic stations. \n",
    "- Weekend, Holiday and Business Day availability patterns will be the most distinct for residential and business stations, whereas touristic stations will be less affected by type of day.\n",
    "- On evenings where inclement weather (in form of rain) was observed, there will be an increase in morning usage of Citi Bikes (by individuals who own bicycles, but prefer to take a Citi Bike into work to avoid having to ride home in the rain). \n",
    "- On business days before and after a holiday, there may be a decrease in overall Citi Bike usage. Might be worth excluding these days entirely from the analysis as they do no represent business days or weekends. Luckily, we only have the 4th of July to deal with as part of this dataset. \n",
    "\n",
    "\n",
    "#### Other Notes\n",
    "- When there is only one bike left at 'residential' or 'business' stations, it may not be a major issue, but it is a problem when it happens at stations classified as 'touristic', assuming that tourists are seldom alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing Complexity\n",
    "First things first, I wanted to reduce the number of stations I was analyzing. The `availability` table resulted in nearly 6 million rows after 2 months, so I decided to export a subset of \"interesting\" stations to begin analyzing. Below is the query I used to find the interesting stations, based on whether there is a high variability in number of bikes, that the bikes regularly get refilled, and that the station has a decent number of bikes. I also limited the number of stations per neighborhood to 1.  \n",
    "https://gist.github.com/alhankeser/9fbaf67a8ce052de72f22ab1630cd91c\n",
    "\n",
    "```sql\n",
    "with variability as (\n",
    "    select\n",
    "\t\tborough,\n",
    "\t\thood,\n",
    "\t\tstation_name,\n",
    "\t\tstation_id,\n",
    "\t\tmax(available_bikes) as max_bikes,\n",
    "\t\tsum(case when available_bikes = 0 then 1 else 0 end) as times_no_bikes,\n",
    "\t\tsum(case when available_docks = 0 then 1 else 0 end) as times_replenished\n",
    "\tfrom \n",
    "\t\tavailability\n",
    "\twhere\n",
    "\t\tstation_status = 'In Service'\n",
    "\tgroup by\n",
    "\t\tstation_id, station_name, hood, borough\n",
    "),\n",
    "percentiles as ( \n",
    "\tselect\n",
    "\t\t*,\n",
    "        ntile(100) over (order by max_bikes asc) max_bikes_percentile,\n",
    "\t\tntile(100) over (order by times_no_bikes asc) no_bikes_percentile,\n",
    "\t \tntile(100) over (order by times_replenished asc) times_replenished_percentile\n",
    "\tfrom\n",
    "\t\tvariability\n",
    "\torder by times_no_bikes\n",
    "),\n",
    "ranks as (\n",
    "\tselect\n",
    "\t\t*,\n",
    "\t\t(max_bikes_percentile + no_bikes_percentile + times_replenished_percentile) as score,\n",
    "\t\trank() over (partition by hood order by (max_bikes_percentile + no_bikes_percentile + times_replenished_percentile) desc) as rank\n",
    "\tfrom \n",
    "\t\tpercentiles\n",
    "\twhere\n",
    "\t\tmax_bikes_percentile > 40 \n",
    "\t\tand no_bikes_percentile > 50\n",
    "\t\tand times_replenished_percentile > 50\n",
    "),\n",
    "ranked_by_hood as (\n",
    "\tselect\n",
    "\t\t*\n",
    "\tfrom \n",
    "\t\tranks\n",
    "\twhere\n",
    "\t\trank = 1 \n",
    "\torder by\n",
    "\t\tscore desc\n",
    ")\n",
    "select\n",
    "\ta.*\n",
    "from\n",
    "\tavailability as a\n",
    "join \n",
    "\tranked_by_hood as rbh \n",
    "\ton a.station_id = rbh.station_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query above reduced the nearly 6 million rows down to 186,000. The csv export used for the analysis below can be [found here](https://github.com/alhankeser/citibike-analysis/blob/master/input/availability_interesting_original.csv). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['time_interval', 'updated_at', 'created_at']\n",
    "df = pd.read_csv('../input/availability_interesting_original.csv', parse_dates=date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186030 entries, 0 to 186029\n",
      "Data columns (total 21 columns):\n",
      "station_id          186030 non-null int64\n",
      "station_name        186030 non-null object\n",
      "station_status      186030 non-null object\n",
      "latitude            186030 non-null float64\n",
      "longitude           186030 non-null float64\n",
      "zip                 186030 non-null int64\n",
      "borough             186030 non-null object\n",
      "hood                186030 non-null object\n",
      "available_bikes     186030 non-null int64\n",
      "available_docks     186030 non-null int64\n",
      "time_interval       186030 non-null datetime64[ns]\n",
      "created_at          186030 non-null datetime64[ns]\n",
      "weather_summary     88053 non-null object\n",
      "precip_intensity    88053 non-null float64\n",
      "temperature         88053 non-null float64\n",
      "humidity            88053 non-null float64\n",
      "wind_speed          88053 non-null float64\n",
      "wind_gust           88053 non-null float64\n",
      "cloud_cover         88053 non-null float64\n",
      "weather_status      88053 non-null object\n",
      "updated_at          186030 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](3), float64(8), int64(4), object(6)\n",
      "memory usage: 29.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station_id station_name station_status   latitude  longitude   zip  \\\n",
      "0        3195      Sip Ave     In Service  40.730897 -74.063913  7306   \n",
      "1        3195      Sip Ave     In Service  40.730897 -74.063913  7306   \n",
      "2        3195      Sip Ave     In Service  40.730897 -74.063913  7306   \n",
      "\n",
      "      borough            hood  available_bikes  available_docks  \\\n",
      "0  New Jersey  Journal Square                1               33   \n",
      "1  New Jersey  Journal Square                0               34   \n",
      "2  New Jersey  Journal Square                0               34   \n",
      "\n",
      "        time_interval          created_at weather_summary  precip_intensity  \\\n",
      "0 2019-05-13 02:45:00 2019-05-13 02:45:04        Overcast               0.0   \n",
      "1 2019-05-13 02:30:00 2019-05-13 02:30:04        Overcast               0.0   \n",
      "2 2019-05-13 02:15:00 2019-05-13 02:15:05        Overcast               0.0   \n",
      "\n",
      "   temperature  humidity  wind_speed  wind_gust  cloud_cover weather_status  \\\n",
      "0        44.86      0.91        6.85       9.65          1.0      predicted   \n",
      "1        44.86      0.91        6.85       9.65          1.0      predicted   \n",
      "2        44.86      0.91        6.85       9.65          1.0      predicted   \n",
      "\n",
      "           updated_at  \n",
      "0 2019-05-13 02:45:04  \n",
      "1 2019-05-13 02:45:04  \n",
      "2 2019-05-13 02:45:04  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(3)) #printing to improve how this looks in the README.md markdown file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Issues\n",
    "Without much digging, it's easy to spot some data quality/consistency issues: \n",
    "1. `weather_status` should be 'observed' for all locations rather than 'predicted' since the dates are in the past. \n",
    "1. `zip` is being converted to an integer and thus dropping the 0, which is may or may not be an issue. If wish to solve problem #1 then this chould be an issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing the zip data_type issue\n",
    "The issue related to zip codes is related to New Jersey's that start with a zero. There are two options to fix this:  \n",
    "1. Mutate the existing column to a string and insert a 0 to the beginning of the incorrect zip.  \n",
    "OR  \n",
    "2. Read the csv with dtype specified as `str` for the `zip` column. \n",
    "\n",
    "Going to go with option #2 and re-import the csv correctly, then check that `zip` is in fact treated as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['time_interval', 'updated_at', 'created_at']\n",
    "data_types = {'zip': str}\n",
    "df = pd.read_csv('../input/availability_interesting_original.csv', parse_dates=date_cols, dtype=data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['zip'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing the missing weather issue\n",
    "As for why weather_status is not set to 'observed', which would mean that the weather data my be inaccurate (since only the predicted weather was captured), I will need to first measure the extent of the problem, then remedy by fetching the correct weather data.\n",
    "\n",
    "First, let's get an understanding of the rows affected: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['predicted' 'observed' nan]\n"
     ]
    }
   ],
   "source": [
    "print(df['weather_status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Status: \n",
      "6448 3.0%  Predicted\n",
      "81605 44.0%  Observed\n",
      "97977 53.0% NAs\n"
     ]
    }
   ],
   "source": [
    "print('Weather Status: ')\n",
    "print(df[df['weather_status'] == 'predicted'].count()[0], str(round(df[df['weather_status'] == 'predicted'].count()[0]/df.count()[0]*100)) + '%' , ' Predicted')\n",
    "print(df[df['weather_status'] == 'observed'].count()[0], str(round(df[df['weather_status'] == 'observed'].count()[0]/df.count()[0]*100)) + '%', ' Observed')\n",
    "print(df[df['weather_status'].isna()].count()[0], str(round(df[df['weather_status'].isna()].count()[0]/df.count()[0]*100)) + '%'' NAs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is apparent that there is a larger issue here: we have `nan` values in the `weather_status` column! Let's assume that there is no option to go back and re-run the original commands that fetched the weather data in the first place and that I will have to do this all here...\n",
    "\n",
    "Let's see which stations are affected by the missing weather_status data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UES' 'Williamsburg' 'LIC' 'New York County' 'Yorkville' 'UWS'\n",
      " 'Journal Square' 'Park Slope' 'Downtown Brooklyn' 'Chelsea'\n",
      " 'Prospect Heights' 'Crown Heights' 'Lincoln Square' 'Alphabet City'\n",
      " 'Canal Street' 'Financial District' 'Little Italy' 'Tribeca'\n",
      " 'Ukrainian Village' 'Battery Park City' 'West Village' 'Clinton Hill'\n",
      " 'Lower East Side' \"Hell's Kitchen\" 'Midtown East' 'Peter Cooper Village'\n",
      " 'Stuyvesant Town-Peter Cooper Village']\n"
     ]
    }
   ],
   "source": [
    "print(df[df['weather_status'].isna()]['hood'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10075' '11249' '11101' '10022' '10028' '10024' '07306' '11215' '11201'\n",
      " '10011' '11238' '10023' '10009' '10013' '10004' '10007' '10003' '10282'\n",
      " '10014' '11205' '10002' '10036' '10010']\n"
     ]
    }
   ],
   "source": [
    "print(df[df['weather_status'].isna()]['zip'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4928\n"
     ]
    }
   ],
   "source": [
    "print(len(df[df['weather_status'].isna()]['time_interval'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  ['2019-05-21T11:45:00.000000000']\n",
      "Finish:  ['2019-07-12T03:45:00.000000000']\n"
     ]
    }
   ],
   "source": [
    "print('Start: ', df[df['weather_status'].isna()][['time_interval']].values[0])\n",
    "print('Finish: ', df[df['weather_status'].isna()][['time_interval']].values[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to get weather data at the hour-level to keep things reasonable when fetching weather data, so I am going to round down the `time_interval` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_hour'] = df['time_interval'].apply(lambda x: x.replace(microsecond=0, second=0, minute=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it worked as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     zip       time_interval           time_hour\n",
      "0  07306 2019-05-13 02:45:00 2019-05-13 02:00:00\n",
      "1  07306 2019-05-13 02:30:00 2019-05-13 02:00:00\n",
      "2  07306 2019-05-13 02:15:00 2019-05-13 02:00:00\n",
      "3  07306 2019-05-13 02:00:00 2019-05-13 02:00:00\n",
      "4  07306 2019-05-13 03:30:00 2019-05-13 03:00:00\n"
     ]
    }
   ],
   "source": [
    "print(df[['zip', 'time_interval','time_hour']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `nan`'s and `predicted`'s in `weather_status`, which are the unique `zip` and `time_hour` combinations that we'll need to re-fetch data for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         zip           time_hour\n",
      "0      07306 2019-05-13 02:00:00\n",
      "12809  11101 2019-05-13 02:00:00\n",
      "13452  11201 2019-05-13 02:00:00\n",
      "887    10003 2019-05-13 02:00:00\n",
      "5502   10013 2019-05-13 02:00:00\n",
      "Rows: 22566\n"
     ]
    }
   ],
   "source": [
    "df_weather_na = df[(df['weather_status'].isna()) | (df['weather_status'] == 'predicted')][['zip','time_hour']].sort_values('time_hour').drop_duplicates()\n",
    "print(df_weather_na.head())\n",
    "print('Rows:', len(df_weather_na))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way that the [Dark Sky Weather API](https://darksky.net/dev/docs) works is that you can fetch a whole day's worth of data for each location and it's considered one request. So rather than making an individual request for each zip and hour combination, I will be making a request for every zip and _day_ combination. Let's further reduce the granularity of the table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         zip           time_hour   time_day\n",
      "0      07306 2019-05-13 02:00:00 2019-05-13\n",
      "12809  11101 2019-05-13 02:00:00 2019-05-13\n",
      "13452  11201 2019-05-13 02:00:00 2019-05-13\n",
      "887    10003 2019-05-13 02:00:00 2019-05-13\n",
      "5502   10013 2019-05-13 02:00:00 2019-05-13\n",
      "Rows: 22566\n"
     ]
    }
   ],
   "source": [
    "df_weather_na['time_day'] = df_weather_na['time_hour'].apply(lambda x: x.replace(hour=0))\n",
    "print(df_weather_na.head())\n",
    "print('Rows:', len(df_weather_na))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need `time_hour`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_na.drop('time_hour', axis=1, inplace=True)\n",
    "df_weather_na = df_weather_na.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         zip   time_day\n",
      "0      07306 2019-05-13\n",
      "12809  11101 2019-05-13\n",
      "13452  11201 2019-05-13\n",
      "887    10003 2019-05-13\n",
      "5502   10013 2019-05-13\n",
      "Rows: 1422\n"
     ]
    }
   ],
   "source": [
    "print(df_weather_na.head())\n",
    "print('Rows:', len(df_weather_na))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the Dark Sky Weather API, we'll need to have sample lat/long coordinates to send. To do so, I will grab the coordinates of one station within each zip to represent that zip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         zip   time_day   latitude  longitude\n",
      "0      07306 2019-05-13  40.730897 -74.063913\n",
      "12809  11101 2019-05-13  40.742327 -73.954117\n",
      "13452  11201 2019-05-13  40.692418 -73.989495\n",
      "887    10003 2019-05-13  40.729538 -73.984267\n",
      "5502   10013 2019-05-13  40.719105 -73.999733\n"
     ]
    }
   ],
   "source": [
    "df_weather_na['latitude'] = df_weather_na['zip'].apply(lambda x: df[df['zip'] == x]['latitude'].unique()[0])\n",
    "df_weather_na['longitude'] = df_weather_na['zip'].apply(lambda x: df[df['zip'] == x]['longitude'].unique()[0])\n",
    "print(df_weather_na.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at what I did above, there's a less resource-intensive way to do that. First, by creating a table of zip to coordinate combinations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        zip   latitude  longitude  order\n",
      "5495  10013  40.719105 -73.999733    1.0\n",
      "5499  10013  40.719392 -74.002472    2.0\n",
      "7077  10014  40.736529 -74.006180    1.0\n",
      "7640  10022  40.757148 -73.972078    1.0\n",
      "7636  10022  40.763505 -73.971092    2.0\n",
      "8960  10023  40.775160 -73.989187    1.0\n"
     ]
    }
   ],
   "source": [
    "df_zip_coord = df[['zip','latitude', 'longitude']].drop_duplicates()\n",
    "df_zip_coord['order'] = df_zip_coord.groupby('zip').latitude.rank(method='min')\n",
    "print(df_zip_coord.sort_values('zip')[9:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        zip   latitude  longitude\n",
      "0     07306  40.730897 -74.063913\n",
      "224   10002  40.720664 -73.985180\n",
      "886   10003  40.729538 -73.984267\n",
      "1548  10004  40.703652 -74.011678\n",
      "2210  10007  40.714979 -74.013012\n"
     ]
    }
   ],
   "source": [
    "df_zip_coord = df_zip_coord[df_zip_coord['order'] == 1]\n",
    "df_zip_coord.drop('order', inplace=True, axis=1)\n",
    "print(df_zip_coord.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Original method timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time Elapsed:  42.9605\n"
     ]
    }
   ],
   "source": [
    "start_time_1 = time.process_time()\n",
    "df_weather_na['latitude'] = df_weather_na['zip'].apply(lambda x: df[df['zip'] == x]['latitude'].unique()[0])\n",
    "df_weather_na['longitude'] = df_weather_na['zip'].apply(lambda x: df[df['zip'] == x]['longitude'].unique()[0])\n",
    "end_time_1 = time.process_time()\n",
    "elapsed_time_1 = round(end_time_1 - start_time_1, 4)\n",
    "print('Process Time Elapsed: ', elapsed_time_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Faster method timing (using lookup table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time Elapsed:  1.5525\n",
      "(~28X faster than method 1)\n"
     ]
    }
   ],
   "source": [
    "start_time_2 = time.process_time()\n",
    "df_weather_na['latitude'] = df_weather_na['zip'].apply(lambda x: df_zip_coord[df_zip_coord['zip'] == x]['latitude'].get_values()[0])\n",
    "df_weather_na['longitude'] = df_weather_na['zip'].apply(lambda x: df_zip_coord[df_zip_coord['zip'] == x]['longitude'].get_values()[0])\n",
    "end_time_2 = time.process_time()\n",
    "elapsed_time_2 = round(end_time_2 - start_time_2, 4)\n",
    "print('Process Time Elapsed: ', elapsed_time_2)\n",
    "print('(~' + str(round(elapsed_time_1/elapsed_time_2)) + 'X faster than method 1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. And an even faster method, now that we have the lookup table, is to simply merge: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Time Elapsed:  0.0046\n",
      "(~338X faster than method 2)\n",
      "(~9339X faster than method 1)\n"
     ]
    }
   ],
   "source": [
    "start_time_3 = time.process_time()\n",
    "df_weather_na = df_weather_na.merge(df_zip_coord, how='inner', on='zip')\n",
    "end_time_3 = time.process_time()\n",
    "elapsed_time_3 = round(end_time_3 - start_time_3, 4)\n",
    "print('Process Time Elapsed: ', elapsed_time_3)\n",
    "print('(~' + str(round(elapsed_time_2/elapsed_time_3)) + 'X faster than method 2)')\n",
    "print('(~' + str(round(elapsed_time_1/elapsed_time_3)) + 'X faster than method 1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-fetching weather data\n",
    "Now that we've reduced the number of individual requests we'll need to make to the [Dark Sky Weather API](https://darksky.net/dev/docs), we can start to setup the re-fetching process. \n",
    "\n",
    "Here is our dataset of missing weather: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     zip   time_day   latitude  longitude\n",
      "0  07306 2019-05-13  40.730897 -74.063913\n",
      "1  07306 2019-05-14  40.730897 -74.063913\n",
      "2  07306 2019-05-15  40.730897 -74.063913\n",
      "3  07306 2019-05-16  40.730897 -74.063913\n",
      "4  07306 2019-05-17  40.730897 -74.063913\n",
      "(1422, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_weather_na.head())\n",
    "print(df_weather_na.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a safety measure to avoid having to through these steps all over again, let's save this dataset to a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_na.to_csv('../input/df_weather_na.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to creating a new DataFrame of correct weather data for the locations and days in `df_weather_na`, by hour. \n",
    "\n",
    "1. Get the api key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds_key = os.getenv(\"DARK_SKY_API_KEY\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a function to build a DataFrame over time. Since I need to regulate how often I call the DS API within a 24h period, I created a set of functions to allow me to pick up where I leave off with no wasted API calls. After some trial and error, I've got it working below. Comments on each function provide more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weather_fix to csv after getting weather for 10007 on 2019-06-30\n",
      "Saved weather_fix to csv after getting weather for 10007 on 2019-07-10\n",
      "Saved weather_fix to csv after getting weather for 10007 on 2019-07-13\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "def get_weather_by_day(api_key, row):\n",
    "    \"\"\"Call the DarkSky API to request single day of weather and return a Forecast object\"\"\"\n",
    "    day = dt.strftime(row.time_day, '%Y-%m-%dT%H:%M:%S')\n",
    "    lat = row.latitude\n",
    "    long = row.longitude\n",
    "    weather = forecast(api_key, lat, long, time=day)\n",
    "    return weather\n",
    "\n",
    "def create_weather_df(zip_code, weather):\n",
    "    \"\"\"Create a DataFrame with the same columns and naming convention as primary df\"\"\"\n",
    "    df_weather = pd.DataFrame(weather['hourly']['data'])\n",
    "    df_weather.rename(columns={ 'precipIntensity': 'precip_intensity',\n",
    "                                'windSpeed': 'wind_speed',\n",
    "                                'windGust': 'wind_gust',\n",
    "                                'cloudCover': 'cloud_cover',\n",
    "                                'summary': 'weather_summary',\n",
    "                                'time': 'time_hour'\n",
    "                                }, inplace=True)\n",
    "    df_weather = df_weather[['time_hour','precip_intensity','temperature',\\\n",
    "                             'humidity', 'wind_speed', 'wind_gust', \\\n",
    "                             'weather_summary', 'cloud_cover']]\n",
    "    df_weather['time_hour'] = df_weather['time_hour'].apply(lambda x: dt.utcfromtimestamp(x-14400).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    df_weather['time_hour'] = df_weather['time_hour'].apply(lambda x: parse(x))\n",
    "    df_weather['zip'] = str(zip_code)\n",
    "    df_weather['weather_status'] = 'observed'\n",
    "    return df_weather\n",
    "\n",
    "def get_start_index_df():\n",
    "    \"\"\"Get the latest version of the fixed weather df and the index on which to start\"\"\"\n",
    "    try:\n",
    "        df_weather_fix = pd.read_csv('../input/df_weather_fix.csv', dtype={'zip': str})\n",
    "        df_weather_filtered = df_weather_na[(df_weather_na['zip'] == df_weather_fix.iloc[-24]['zip'])\\\n",
    "                                        & (df_weather_na['time_day'] == df_weather_fix.iloc[-24]['time_hour'])]\n",
    "        return df_weather_fix, df_weather_filtered.index[0]+1\n",
    "    except:\n",
    "        df_weather_fix = pd.DataFrame(columns=['time_hour', 'precip_intensity',\\\n",
    "                                       'temperature', 'humidity',\\\n",
    "                                       'wind_speed', 'wind_gust',\\\n",
    "                                       'weather_summary', 'cloud_cover',\\\n",
    "                                       'zip', 'weather_status'])\n",
    "        return df_weather_fix, 0\n",
    "    \n",
    "def get_weather_fix(ds_key, api_limit, df_weather_na):\n",
    "    \"\"\"Create an on-going df which contains missing weather data in increments as defined in api_limit\"\"\"\n",
    "    df_weather_fix, start_index = get_start_index_df()\n",
    "    api_limit = api_limit\n",
    "    for index, row in df_weather_na[start_index:].iterrows():\n",
    "        if index < (api_limit + start_index):\n",
    "            weather = get_weather_by_day(ds_key, row)\n",
    "            df_weather_day = create_weather_df(row['zip'], weather)\n",
    "            df_weather_fix = df_weather_fix.append(df_weather_day)\n",
    "        else:\n",
    "            df_weather_fix.to_csv('../input/df_weather_fix.csv', index=False)\n",
    "            print('Saved weather_fix to csv after', 'getting weather for', row['zip'], 'on', str(row['time_day']).split(' ')[0])\n",
    "            break\n",
    "        if index % 10 == 0:\n",
    "            df_weather_fix.to_csv('../input/df_weather_fix.csv', index=False)\n",
    "            print('Saved weather_fix to csv after', 'getting weather for', row['zip'], 'on', str(row['time_day']).split(' ')[0])\n",
    "    print('DONE!')\n",
    "    \n",
    "get_weather_fix(ds_key, 13, df_weather_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I successfully kept the total API requests to just below 1000 for today:\n",
    "![Dark Sky API Calls today](https://blog.alhan.co/storage/images/posts/2/dark-sky-api-calls_2_1569210787_md.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `IN PROGRESS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>station_status</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>zip</th>\n",
       "      <th>borough</th>\n",
       "      <th>hood</th>\n",
       "      <th>available_bikes</th>\n",
       "      <th>available_docks</th>\n",
       "      <th>time_interval</th>\n",
       "      <th>created_at</th>\n",
       "      <th>weather_summary_x</th>\n",
       "      <th>precip_intensity_x</th>\n",
       "      <th>temperature_x</th>\n",
       "      <th>humidity_x</th>\n",
       "      <th>wind_speed_x</th>\n",
       "      <th>wind_gust_x</th>\n",
       "      <th>cloud_cover_x</th>\n",
       "      <th>weather_status_x</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>time_hour</th>\n",
       "      <th>precip_intensity_y</th>\n",
       "      <th>temperature_y</th>\n",
       "      <th>humidity_y</th>\n",
       "      <th>wind_speed_y</th>\n",
       "      <th>wind_gust_y</th>\n",
       "      <th>weather_summary_y</th>\n",
       "      <th>cloud_cover_y</th>\n",
       "      <th>weather_status_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3195</td>\n",
       "      <td>Sip Ave</td>\n",
       "      <td>In Service</td>\n",
       "      <td>40.730897</td>\n",
       "      <td>-74.063913</td>\n",
       "      <td>07306</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Journal Square</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2019-05-13 02:45:00</td>\n",
       "      <td>2019-05-13 02:45:04</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.86</td>\n",
       "      <td>0.91</td>\n",
       "      <td>6.85</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>predicted</td>\n",
       "      <td>2019-05-13 02:45:04</td>\n",
       "      <td>2019-05-13 02:00:00</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.69</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>1.0</td>\n",
       "      <td>observed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3195</td>\n",
       "      <td>Sip Ave</td>\n",
       "      <td>In Service</td>\n",
       "      <td>40.730897</td>\n",
       "      <td>-74.063913</td>\n",
       "      <td>07306</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Journal Square</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>2019-05-13 02:30:00</td>\n",
       "      <td>2019-05-13 02:30:04</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.86</td>\n",
       "      <td>0.91</td>\n",
       "      <td>6.85</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>predicted</td>\n",
       "      <td>2019-05-13 02:45:04</td>\n",
       "      <td>2019-05-13 02:00:00</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.69</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>1.0</td>\n",
       "      <td>observed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3195</td>\n",
       "      <td>Sip Ave</td>\n",
       "      <td>In Service</td>\n",
       "      <td>40.730897</td>\n",
       "      <td>-74.063913</td>\n",
       "      <td>07306</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Journal Square</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>2019-05-13 02:15:00</td>\n",
       "      <td>2019-05-13 02:15:05</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.86</td>\n",
       "      <td>0.91</td>\n",
       "      <td>6.85</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>predicted</td>\n",
       "      <td>2019-05-13 02:45:04</td>\n",
       "      <td>2019-05-13 02:00:00</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.69</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>1.0</td>\n",
       "      <td>observed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3195</td>\n",
       "      <td>Sip Ave</td>\n",
       "      <td>In Service</td>\n",
       "      <td>40.730897</td>\n",
       "      <td>-74.063913</td>\n",
       "      <td>07306</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Journal Square</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>2019-05-13 02:00:00</td>\n",
       "      <td>2019-05-13 02:00:05</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.86</td>\n",
       "      <td>0.91</td>\n",
       "      <td>6.85</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>predicted</td>\n",
       "      <td>2019-05-13 02:45:04</td>\n",
       "      <td>2019-05-13 02:00:00</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.69</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>1.0</td>\n",
       "      <td>observed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id station_name station_status   latitude  longitude    zip  \\\n",
       "0        3195      Sip Ave     In Service  40.730897 -74.063913  07306   \n",
       "1        3195      Sip Ave     In Service  40.730897 -74.063913  07306   \n",
       "2        3195      Sip Ave     In Service  40.730897 -74.063913  07306   \n",
       "3        3195      Sip Ave     In Service  40.730897 -74.063913  07306   \n",
       "\n",
       "      borough            hood  available_bikes  available_docks  \\\n",
       "0  New Jersey  Journal Square                1               33   \n",
       "1  New Jersey  Journal Square                0               34   \n",
       "2  New Jersey  Journal Square                0               34   \n",
       "3  New Jersey  Journal Square                0               34   \n",
       "\n",
       "        time_interval          created_at weather_summary_x  \\\n",
       "0 2019-05-13 02:45:00 2019-05-13 02:45:04          Overcast   \n",
       "1 2019-05-13 02:30:00 2019-05-13 02:30:04          Overcast   \n",
       "2 2019-05-13 02:15:00 2019-05-13 02:15:05          Overcast   \n",
       "3 2019-05-13 02:00:00 2019-05-13 02:00:05          Overcast   \n",
       "\n",
       "   precip_intensity_x  temperature_x  humidity_x  wind_speed_x  wind_gust_x  \\\n",
       "0                 0.0          44.86        0.91          6.85         9.65   \n",
       "1                 0.0          44.86        0.91          6.85         9.65   \n",
       "2                 0.0          44.86        0.91          6.85         9.65   \n",
       "3                 0.0          44.86        0.91          6.85         9.65   \n",
       "\n",
       "   cloud_cover_x weather_status_x          updated_at           time_hour  \\\n",
       "0            1.0        predicted 2019-05-13 02:45:04 2019-05-13 02:00:00   \n",
       "1            1.0        predicted 2019-05-13 02:45:04 2019-05-13 02:00:00   \n",
       "2            1.0        predicted 2019-05-13 02:45:04 2019-05-13 02:00:00   \n",
       "3            1.0        predicted 2019-05-13 02:45:04 2019-05-13 02:00:00   \n",
       "\n",
       "   precip_intensity_y  temperature_y  humidity_y  wind_speed_y  wind_gust_y  \\\n",
       "0              0.0033           43.0        0.92          7.69         7.69   \n",
       "1              0.0033           43.0        0.92          7.69         7.69   \n",
       "2              0.0033           43.0        0.92          7.69         7.69   \n",
       "3              0.0033           43.0        0.92          7.69         7.69   \n",
       "\n",
       "  weather_summary_y  cloud_cover_y weather_status_y  \n",
       "0          Overcast            1.0         observed  \n",
       "1          Overcast            1.0         observed  \n",
       "2          Overcast            1.0         observed  \n",
       "3          Overcast            1.0         observed  "
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather_fix = pd.read_csv('../input/df_weather_fix.csv', dtype={'zip': str})\n",
    "df_copy = df.copy(deep=True)\n",
    "df_copy = df_copy.merge(df_weather, how='left', on=['time_hour', 'zip'])\n",
    "df_copy[(df_copy['zip'] == '07306') & \\\n",
    "              (df_copy['time_hour'] == '2019-05-13 02:00:00')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-Generate README.md:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook analysis.ipynb to markdown\n",
      "[NbConvertApp] Writing 38078 bytes to ../README.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --output-dir='..' --to markdown analysis.ipynb --output README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Powered by Dark Sky](https://darksky.net/poweredby/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
